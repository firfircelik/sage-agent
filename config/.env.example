# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Groq Configuration
GROQ_API_KEY=your_groq_api_key_here

# Cohere Configuration
COHERE_API_KEY=your_cohere_api_key_here

# Mistral Configuration
MISTRAL_API_KEY=your_mistral_api_key_here

# DeepSeek Configuration
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# GLM (Zhipu) Configuration
GLM_API_KEY=your_glm_api_key_here

# LLM Provider Selection
# Options: openai, anthropic, local, groq, cohere, mistral, deepseek, glm, opencode
LLM_PROVIDER=openai

# Local LLM Configuration (if using local provider)
LOCAL_LLM_BASE_URL=http://localhost:11434
LOCAL_LLM_MODEL=llama2

# RLM Configuration
RLM_CACHE_DIR=.rlm_cache
RLM_ENABLED=true
RLM_CACHE_RESPONSES=true
